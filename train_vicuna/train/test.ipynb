{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caiosilva/Documentos/dell-adv/tuning_vicuna/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers.trainer_pt_utils import LabelSmoother #código para evitar overconfidence no modelo\n",
    "from fastchat.conversation import SeparatorStyle\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.85s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'helloollel/vicuna-7b'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name\n",
    "    )\n",
    "model.config.use_cache = False\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000000019884624838656"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_rank = None\n",
    "\n",
    "def rank0_print(*args):\n",
    "    if local_rank==0:\n",
    "        print(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = json.load(open(\"dummy.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_source = []\n",
    "for i in source:\n",
    "    new_conv = []\n",
    "    for j in i['conversations']:\n",
    "        if j['from'] == 'human':\n",
    "            new_conv.append({\"from\":\"client\", \"value\":j['value']})\n",
    "        else:\n",
    "            new_conv.append({\"from\":\"agent\", \"value\":j['value']})\n",
    "    new_source.append(new_conv)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeparatorStyle.ADD_COLON_TWO\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   319, 13563,  ...,     2,     2,     2],\n",
       "         [    1,   319, 13563,  ...,     2,     2,     2],\n",
       "         [    1,   319, 13563,  ...,     2,     2,     2],\n",
       "         ...,\n",
       "         [    1,   319, 13563,  ...,     2,     2,     2],\n",
       "         [    1,   319, 13563,  ...,     2,     2,     2],\n",
       "         [    1,   319, 13563,  ...,     2,     2,     2]]),\n",
       " 'labels': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "preprocess(new_source,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sources,\n",
    "               tokenizer: transformers.PreTrainedTokenizer,\n",
    "               ) -> Dict:\n",
    "    \"\"\"Preprocesses the data into a format suitable for training.\"\"\"\n",
    "    conv = get_conversation_template(\"vicuna\")\n",
    "    roles = {\"client\":conv.roles[0],\"agent\":conv.roles[1]}\n",
    "\n",
    "    #Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "             # Skip the first one if it is not from client\n",
    "             source = source[1:]\n",
    "\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j%2], f\"{i}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    # Tokenize conversations\n",
    "    input_ids = tokenizer(\n",
    "        conversations,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    targets = input_ids.clone()\n",
    "    \n",
    "    assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO\n",
    "    \n",
    "    #Mask targets\n",
    "    sep = conv.sep + conv.roles[1] + \": \"\n",
    "    for conversation, target in zip(conversations,targets):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "        rounds = conversation.split(conv.sep2)\n",
    "        cur_len = 1\n",
    "        target[:cur_len] = IGNORE_TOKEN_ID\n",
    "\n",
    "        for i, rou in enumerate(rounds):\n",
    "            if rou ==\"\":\n",
    "                break\n",
    "            \n",
    "            parts = rou.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            \n",
    "            parts[0] +=sep\n",
    "            round_len = len(tokenizer(rou).input_ids)\n",
    "            instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n",
    "            \n",
    "            cur_len += round_len\n",
    "        target[cur_len:] = IGNORE_TOKEN_ID\n",
    "\n",
    "        if False:\n",
    "            z = target.clone()\n",
    "            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id,z)\n",
    "            rank0_print(tokenizer.decode(z))\n",
    "        \n",
    "        if cur_len < tokenizer.model_max_length:\n",
    "            if cur_len != total_len:\n",
    "                target[:] = IGNORE_TOKEN_ID\n",
    "                rank0_print(\n",
    "                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}\"\n",
    "                    f\"(ignored)\"\n",
    "                )\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "        attention_mask = input_ids.ne(tokenizer.pad_token_id),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tuning_vicuna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
